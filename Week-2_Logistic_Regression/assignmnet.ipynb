{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "## Logistic Regression \n",
    "Logistic Regression is a fundamental supervised learning algorithm used for binary classification tasks. Despite its name, it's used for classification, not regression. Logistic Regression models the probability that a given input belongs to a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Steps\n",
    "1. Data Pre-processing\n",
    "2. Parameter Initialization\n",
    "3. Forward Propagation:\n",
    "4. Cost Calculation\n",
    "5. Backward Propagation\n",
    "6. Parameter Update\n",
    "7. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import libraries we've already learnt and will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Pre-Processing\n",
    "The dataset contains information that may be used for diagnosing or predicting the presence of heart disease in individuals. It comprises various clinical and demographic features that are commonly considered in cardiovascular health assessment. Understanding and analyzing these features can aid in developing predictive models or understanding the factors associated with heart disease.  \n",
    "The first 13 fields are \n",
    "- Age\n",
    "- Sex\n",
    "- Chest Pain Type (cp)\n",
    "- Resting Blood Pressure (trestbps)\n",
    "- Serum Cholesterol Level (chol)\n",
    "- Fasting Blood Sugar (fbs)\n",
    "- Resting Electrocardiographic Results (restecg)\n",
    "- Maximum Heart Rate Achieved (thalach)\n",
    "- Exercise-Induced Angina (exang)\n",
    "- ST Depression Induced by Exercise Relative to Rest (oldpeak)\n",
    "- Slope of the Peak Exercise ST Segment (slope)\n",
    "- Number of Major Vessels Colored by Fluoroscopy (ca)\n",
    "- Thalassemia (thal)  \n",
    "\n",
    "And the 14th field is 'target' and is either 0 or 1. The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import data from the csv file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1026, 14)\n"
     ]
    }
   ],
   "source": [
    "dataset_raw = np.genfromtxt(\"./dataset/heart.csv\", dtype=\"str\", delimiter=\",\")\n",
    "print(dataset_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset have headers that we don't need for logistic regression model. That is there for us to understand what the values denote  \n",
    "We seperate the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age' 'sex' 'cp' 'trestbps' 'chol' 'fbs' 'restecg' 'thalach' 'exang'\n",
      " 'oldpeak' 'slope' 'ca' 'thal' 'target']\n"
     ]
    }
   ],
   "source": [
    "headers = dataset_raw[0, :]\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we get rest of the numerical data and cast them as `float` data type instead of `string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52.  1.  0. ...  2.  3.  0.]\n",
      " [53.  1.  0. ...  0.  3.  0.]\n",
      " [70.  1.  0. ...  0.  3.  0.]\n",
      " ...\n",
      " [47.  1.  0. ...  1.  2.  0.]\n",
      " [50.  0.  0. ...  0.  2.  1.]\n",
      " [54.  1.  0. ...  1.  3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_raw[1:, :]\n",
    "dataset = dataset.astype(float)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the first 13 columns represent the features, while the 14th column indicates whether the individual has the disease or not based on those features. Here we seperate the dataset into X (feature vector) and Y (output vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1025, 13)\n",
      "(1025,)\n"
     ]
    }
   ],
   "source": [
    "X = dataset[:, :13]\n",
    "Y = dataset[:, 13]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of X here is $(m, n_x)$, but we want it to have $shape = (n_x, m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 1025)\n",
      "(1025,)\n"
     ]
    }
   ],
   "source": [
    "X = X.T\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks good\n",
    "Finally, we have 1025 examples of 13 features and 1 output  \n",
    "From the notation you have studied till now  \n",
    "$$\n",
    "\\begin{align*}\n",
    "n_x &= 13 \\\\\n",
    "n_y &= 1 \\\\\n",
    "m &= 1025\n",
    "\\end{align*}\n",
    "$$\n",
    "Now we can proceed to making our logistic regression model and train our model  \n",
    "But, we have one problem to deal with. How would we know if our model is doing good and if it is, how good is it?  \n",
    "That is why from the 1025 examples we have, we'll keep some data aside and use it to test our model's prediction  \n",
    "Let's keep 80% of the data for training and 20% of the data for testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index to split data in 80:20 ratio\n",
    "index = int(0.8 * X.shape[1])\n",
    "\n",
    "# split the data\n",
    "X_train = X[:, :index]\n",
    "X_test = X[:, index:]\n",
    "\n",
    "Y_train = Y[:index]\n",
    "Y_test = Y[index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print shapes for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (13, 820)\n",
      "Y_train shape (820,)\n",
      "Number of training examples = 820\n",
      "----------------------------------------\n",
      "X_test shape (13, 205)\n",
      "Y_test shape (205,)\n",
      "Number of testing examples = 205\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "print(\"Number of training examples =\", Y_train.shape[0])\n",
    "print(\"-\"*40)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_test shape\", Y_test.shape)\n",
    "print(\"Number of testing examples =\", Y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "#### Forward Propagation\n",
    "$$\n",
    "Z = W^T X + b  \\\\\n",
    "A = sigmoid(X) \\\\\n",
    "$$\n",
    "#### Calculate Cost\n",
    "$$\n",
    "J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "$$\n",
    "#### Backward Propagation\n",
    "$$ \\partial W = \\frac{\\partial J}{\\partial W} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\partial b = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "#### Parameter Updation\n",
    "$$ W = W - \\alpha \\text{ } \\partial W $$\n",
    "$$ b = b - \\alpha \\text{ } \\partial b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initializing parameters\n",
    "**Assignment**: Complete the function for parameter initialization in the cell below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(num_features):\n",
    "  \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "\n",
    "    Returns:\n",
    "    W -- initialized vector of shape (num_features, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "  \"\"\"\n",
    "\n",
    "  W = np.zeros((num_features,1))\n",
    "  b = 0\n",
    "\n",
    "  return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helper Function**: Sigmoid\n",
    "**Assignment**: Complete the function for calculating sigmoid in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "  \"\"\"\n",
    "  \n",
    "  s = 1/(1+np.exp(-x))\n",
    "\n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forward Propagation\n",
    "$$\n",
    "Z = W^T X + b  \\\\\n",
    "A = sigmoid(X) \\\\\n",
    "$$\n",
    "**Assignment**: Complete the function implementing forward propagation in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W, b, X):\n",
    "  \"\"\"\n",
    "    Compute forward propagation\n",
    "\n",
    "    Return:\n",
    "    A -- activation\n",
    "  \"\"\"\n",
    "\n",
    "  # forward propagation\n",
    "  Z = np.dot(W.T,X)+b\n",
    "  A = sigmoid(Z)\n",
    "  \n",
    "  return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Cost\n",
    "$$\n",
    "J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "$$\n",
    "**Assignment**: Complete the function to calculate cost in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(A, Y):\n",
    "  \"\"\"\n",
    "    Calculate cross entropy loss between calculated values (A) and actual values (Y)\n",
    "\n",
    "    Return:\n",
    "    cost -- cost calculated\n",
    "  \"\"\"\n",
    "\n",
    "  # get number of examples\n",
    "  m = Y.shape[0]\n",
    "\n",
    "  # calculate cost\n",
    "  cost = -(1/m)*np.sum(Y.reshape((1,-1))*np.log(A)+(1-Y).reshape((1,-1))*np.log(1-A))\n",
    "\n",
    "  # this will remove any useless dimensions from cost\n",
    "  # not doing this might give us an array instead of a single value\n",
    "  cost = np.squeeze(cost)\n",
    "\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Backward Propagation\n",
    "$$ \\partial W = \\frac{\\partial J}{\\partial W} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\partial b = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "**Assignment**: Complete the function to compute gradients in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(A, X, Y):\n",
    "  \"\"\"\n",
    "    Calculate gradients dW and db\n",
    "\n",
    "    Return:\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "  \"\"\"\n",
    "\n",
    "  # get number of examples\n",
    "  m = Y.shape[0]\n",
    "\n",
    "  # calculate gradients\n",
    "  dW = (1/m)*np.dot(X,(A-Y).T)\n",
    "  db = (1/m)*np.sum(A-Y)\n",
    "\n",
    "  return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Parameter Updation\n",
    "$$ W = W - \\alpha \\text{ } \\partial W $$\n",
    "$$ b = b - \\alpha \\text{ } \\partial b $$\n",
    "**Assignment**: Complete the function to update parameters in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W, b, dW, db, learning_rate):\n",
    "  \"\"\"\n",
    "    Update params W and b from their gradients\n",
    "\n",
    "    Return:\n",
    "    W -- updated W\n",
    "    b -- updated b\n",
    "  \"\"\"\n",
    "\n",
    "  W = W-learning_rate*dW\n",
    "  b = b-learning_rate*db\n",
    "\n",
    "  return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model's forward propagation return value calculated from sigmoid function, it lies between 0 and 1  \n",
    "We can say output should be 1 when A > 0.5 and 0 when A < 0.5  \n",
    "Let's implement a function that will do this for us in vectorized way\n",
    "### Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "  A = forward_prop(W, b, X)\n",
    "  Y_pred = (A>=0.5)*1.0\n",
    "  return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compile all these function to implement training loop for our model\n",
    "You don't need to write any math here, just put all the functions you've written above in the right sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, num_iterations=10000, learning_rate=0.01, print_cost=True):\n",
    "  # initialize parameters\n",
    "  W, b = init_params(13)\n",
    "\n",
    "  # let's keep track of our cost to see how our model\n",
    "  # reduces cost after every few iteration\n",
    "  costs = []\n",
    "\n",
    "  for i in range(num_iterations):\n",
    "\n",
    "    # forward propagation\n",
    "    A = forward_prop(W,b,X)\n",
    "\n",
    "    # calculate cost\n",
    "    cost = calculate_loss(A,Y)\n",
    "\n",
    "    # backward propagation\n",
    "    dW, db = backward_prop(A,X,Y)\n",
    "\n",
    "    # parameter updation\n",
    "    W, b = update_params(W,b,dW,db,learning_rate)\n",
    "\n",
    "    # store cost after every few iterations\n",
    "    if i%100 == 0:\n",
    "      costs.append(cost)\n",
    "\n",
    "    # print cost after every few iterations\n",
    "    if print_cost and i%100 == 0:\n",
    "      print(f\"Cost after {i+1} iteration : {cost}\")\n",
    "    \n",
    "  return W, b, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 1 iteration : 0.6931471805599453\n",
      "Cost after 101 iteration : nan\n",
      "Cost after 201 iteration : nan\n",
      "Cost after 301 iteration : nan\n",
      "Cost after 401 iteration : nan\n",
      "Cost after 501 iteration : nan\n",
      "Cost after 601 iteration : nan\n",
      "Cost after 701 iteration : nan\n",
      "Cost after 801 iteration : nan\n",
      "Cost after 901 iteration : nan\n",
      "Cost after 1001 iteration : nan\n",
      "Cost after 1101 iteration : nan\n",
      "Cost after 1201 iteration : nan\n",
      "Cost after 1301 iteration : nan\n",
      "Cost after 1401 iteration : nan\n",
      "Cost after 1501 iteration : nan\n",
      "Cost after 1601 iteration : nan\n",
      "Cost after 1701 iteration : nan\n",
      "Cost after 1801 iteration : nan\n",
      "Cost after 1901 iteration : nan\n",
      "Cost after 2001 iteration : nan\n",
      "Cost after 2101 iteration : nan\n",
      "Cost after 2201 iteration : nan\n",
      "Cost after 2301 iteration : nan\n",
      "Cost after 2401 iteration : nan\n",
      "Cost after 2501 iteration : nan\n",
      "Cost after 2601 iteration : nan\n",
      "Cost after 2701 iteration : nan\n",
      "Cost after 2801 iteration : nan\n",
      "Cost after 2901 iteration : nan\n",
      "Cost after 3001 iteration : nan\n",
      "Cost after 3101 iteration : nan\n",
      "Cost after 3201 iteration : nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/42p0fwpd1zb3g1hbkr2925zc0000gn/T/ipykernel_10355/3613917817.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1/(1+np.exp(-x))\n",
      "/var/folders/0p/42p0fwpd1zb3g1hbkr2925zc0000gn/T/ipykernel_10355/140537933.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -(1/m)*np.sum(Y.reshape((1,-1))*np.log(A)+(1-Y).reshape((1,-1))*np.log(1-A))\n",
      "/var/folders/0p/42p0fwpd1zb3g1hbkr2925zc0000gn/T/ipykernel_10355/140537933.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -(1/m)*np.sum(Y.reshape((1,-1))*np.log(A)+(1-Y).reshape((1,-1))*np.log(1-A))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 3301 iteration : nan\n",
      "Cost after 3401 iteration : nan\n",
      "Cost after 3501 iteration : nan\n",
      "Cost after 3601 iteration : nan\n",
      "Cost after 3701 iteration : nan\n",
      "Cost after 3801 iteration : nan\n",
      "Cost after 3901 iteration : nan\n",
      "Cost after 4001 iteration : nan\n",
      "Cost after 4101 iteration : nan\n",
      "Cost after 4201 iteration : nan\n",
      "Cost after 4301 iteration : nan\n",
      "Cost after 4401 iteration : nan\n",
      "Cost after 4501 iteration : nan\n",
      "Cost after 4601 iteration : nan\n",
      "Cost after 4701 iteration : nan\n",
      "Cost after 4801 iteration : nan\n",
      "Cost after 4901 iteration : nan\n",
      "Cost after 5001 iteration : nan\n",
      "Cost after 5101 iteration : nan\n",
      "Cost after 5201 iteration : nan\n",
      "Cost after 5301 iteration : nan\n",
      "Cost after 5401 iteration : nan\n",
      "Cost after 5501 iteration : nan\n",
      "Cost after 5601 iteration : nan\n",
      "Cost after 5701 iteration : nan\n",
      "Cost after 5801 iteration : nan\n",
      "Cost after 5901 iteration : nan\n",
      "Cost after 6001 iteration : nan\n",
      "Cost after 6101 iteration : nan\n",
      "Cost after 6201 iteration : nan\n",
      "Cost after 6301 iteration : nan\n",
      "Cost after 6401 iteration : nan\n",
      "Cost after 6501 iteration : nan\n",
      "Cost after 6601 iteration : nan\n",
      "Cost after 6701 iteration : nan\n",
      "Cost after 6801 iteration : nan\n",
      "Cost after 6901 iteration : nan\n",
      "Cost after 7001 iteration : nan\n",
      "Cost after 7101 iteration : nan\n",
      "Cost after 7201 iteration : nan\n",
      "Cost after 7301 iteration : nan\n",
      "Cost after 7401 iteration : nan\n",
      "Cost after 7501 iteration : nan\n",
      "Cost after 7601 iteration : nan\n",
      "Cost after 7701 iteration : nan\n",
      "Cost after 7801 iteration : nan\n",
      "Cost after 7901 iteration : nan\n",
      "Cost after 8001 iteration : nan\n",
      "Cost after 8101 iteration : nan\n",
      "Cost after 8201 iteration : nan\n",
      "Cost after 8301 iteration : nan\n",
      "Cost after 8401 iteration : nan\n",
      "Cost after 8501 iteration : nan\n",
      "Cost after 8601 iteration : nan\n",
      "Cost after 8701 iteration : nan\n",
      "Cost after 8801 iteration : nan\n",
      "Cost after 8901 iteration : nan\n",
      "Cost after 9001 iteration : nan\n",
      "Cost after 9101 iteration : nan\n",
      "Cost after 9201 iteration : nan\n",
      "Cost after 9301 iteration : nan\n",
      "Cost after 9401 iteration : nan\n",
      "Cost after 9501 iteration : nan\n",
      "Cost after 9601 iteration : nan\n",
      "Cost after 9701 iteration : nan\n",
      "Cost after 9801 iteration : nan\n",
      "Cost after 9901 iteration : nan\n"
     ]
    }
   ],
   "source": [
    "W, b, costs = train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot costs to see how our model was converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Evaluation\n",
    "Let's check accuracy on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 62.073170731707314 %\n",
      "Test accuracy: 62.4390243902439 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(predict(W, b, X_train) - Y_train)) * 100))\n",
    "print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(predict(W, b, X_test) - Y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Increasing the number of iterations can improve training accuracy, but there's a risk of overfitting. Overfitting occurs when the model learns to fit the training data too closely, capturing noise or irrelevant patterns that don't generalize well to unseen data. In such cases, while the training accuracy continues to increase, the test accuracy may plateau or even decrease.\n",
    "\n",
    "To address overfitting, techniques like regularization and model complexity reduction are employed. Additionally, the transition to deep neural networks offers more sophisticated methods for improving accuracies, even in the presence of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
